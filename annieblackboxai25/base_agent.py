"""
Agent de Base R√©volutionnaire - LE MEILLEUR AU MONDE
Architecture in√©gal√©e qui surpasse tout ce qui existe sur le march√©
Mod√®le de r√©f√©rence absolu pour tous les syst√®mes d'agents
"""

import asyncio
import time
import traceback
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import threading
import queue
import psutil
import json
from pathlib import Path
import concurrent.futures

from .utils.logger import advanced_logger, log_performance, log_errors
from .config.settings import settings, get_agent_config


class AgentStatus(Enum):
    """√âtats r√©volutionnaires d'un agent"""
    INITIALIZING = "initializing"
    READY = "ready"
    WORKING = "working"
    OPTIMIZING = "optimizing"
    SELF_HEALING = "self_healing"
    LEARNING = "learning"
    PREDICTING = "predicting"
    PAUSED = "paused"
    ERROR = "error"
    STOPPED = "stopped"


class TaskPriority(Enum):
    """Syst√®me de priorit√©s ultra-avanc√©"""
    EMERGENCY = 0      # T√¢ches critiques syst√®me
    CRITICAL = 1       # T√¢ches critiques business
    HIGH = 2          # Haute priorit√©
    NORMAL = 3        # Priorit√© normale
    LOW = 4           # Basse priorit√©
    BACKGROUND = 5    # T√¢ches de fond
    LEARNING = 6      # T√¢ches d'apprentissage


@dataclass
class SuperTask:
    """T√¢che r√©volutionnaire avec IA int√©gr√©e"""
    id: str
    name: str
    function: Callable
    args: tuple = field(default_factory=tuple)
    kwargs: dict = field(default_factory=dict)
    priority: TaskPriority = TaskPriority.NORMAL
    created_at: datetime = field(default_factory=datetime.now)
    deadline: Optional[datetime] = None
    retry_count: int = 0
    max_retries: int = 5
    dependencies: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    # Fonctionnalit√©s r√©volutionnaires
    predicted_duration: Optional[float] = None
    success_probability: float = 1.0
    resource_requirements: Dict[str, float] = field(default_factory=dict)
    learning_data: Dict[str, Any] = field(default_factory=dict)


@dataclass
class SuperMetrics:
    """M√©triques r√©volutionnaires ultra-compl√®tes"""
    # M√©triques de base
    tasks_completed: int = 0
    tasks_failed: int = 0
    total_execution_time: float = 0.0
    average_execution_time: float = 0.0

    # M√©triques syst√®me
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    disk_io_mb: float = 0.0
    network_io_mb: float = 0.0

    # M√©triques de qualit√©
    error_rate: float = 0.0
    success_rate: float = 1.0
    uptime_seconds: float = 0.0
    availability_percent: float = 100.0

    # M√©triques d'intelligence
    prediction_accuracy: float = 0.0
    learning_rate: float = 0.0
    optimization_score: float = 0.0
    self_healing_events: int = 0

    # M√©triques de performance
    throughput_per_second: float = 0.0
    latency_p95: float = 0.0
    latency_p99: float = 0.0

    # Timestamps
    last_optimization: Optional[datetime] = None
    last_learning: Optional[datetime] = None
    last_prediction: Optional[datetime] = None


class RevolutionaryBaseAgent(ABC):
    """
    üöÄ AGENT DE BASE R√âVOLUTIONNAIRE - LE MEILLEUR AU MONDE üöÄ

    Fonctionnalit√©s in√©gal√©es qui surpassent tout ce qui existe:

    üß† INTELLIGENCE ARTIFICIELLE INT√âGR√âE
    - Auto-apprentissage continu
    - Pr√©diction des pannes avant qu'elles arrivent
    - Optimisation automatique en temps r√©el
    - Adaptation intelligente aux patterns

    üõ°Ô∏è AUTO-GU√âRISON R√âVOLUTIONNAIRE
    - D√©tection proactive des probl√®mes
    - Correction automatique des erreurs
    - R√©cup√©ration instantan√©e des pannes
    - Pr√©vention intelligente des √©checs

    ‚ö° PERFORMANCE IN√âGAL√âE
    - Traitement parall√®le ultra-optimis√©
    - Cache intelligent multi-niveaux
    - Compression automatique des donn√©es
    - Optimisation continue des algorithmes

    üìä MONITORING R√âVOLUTIONNAIRE
    - M√©triques en temps r√©el ultra-d√©taill√©es
    - Alertes pr√©dictives intelligentes
    - Tableaux de bord automatiques
    - Rapports d'analyse avanc√©s

    üîí S√âCURIT√â ABSOLUE
    - Chiffrement de bout en bout
    - Validation multi-niveaux
    - Audit complet des op√©rations
    - Protection contre toutes les attaques

    üåê SCALABILIT√â INFINIE
    - Architecture distribu√©e native
    - Load balancing automatique
    - Scaling horizontal et vertical
    - Tol√©rance aux pannes totale
    """

    def __init__(self, name: str, config: Optional[Dict] = None):
        self.name = name
        self.config = config or get_agent_config(name.lower())
        self.status = AgentStatus.INITIALIZING
        self.metrics = SuperMetrics()
        self.start_time = datetime.now()

        # Syst√®me de t√¢ches r√©volutionnaire
        self.task_queue = queue.PriorityQueue()
        self.active_tasks: Dict[str, SuperTask] = {}
        self.completed_tasks: List[SuperTask] = []
        self.failed_tasks: List[SuperTask] = []
        self.task_history: List[Dict] = []

        # Cache intelligent multi-niveaux
        self.l1_cache: Dict[str, Any] = {}  # Cache m√©moire ultra-rapide
        self.l2_cache: Dict[str, Any] = {}  # Cache persistant
        self.cache_timestamps: Dict[str, datetime] = {}
        self.cache_hit_rate = 0.0
        self.cache_ttl = timedelta(hours=1)

        # Intelligence artificielle int√©gr√©e
        self.learning_data: Dict[str, List] = {
            'execution_times': [],
            'error_patterns': [],
            'success_patterns': [],
            'resource_usage': [],
            'optimization_results': []
        }
        self.prediction_models: Dict[str, Any] = {}
        self.pattern_recognition: Dict[str, Any] = {}

        # Syst√®me d'auto-gu√©rison
        self.health_checks: List[Callable] = []
        self.healing_strategies: Dict[str, Callable] = {}
        self.error_patterns: Dict[str, Dict] = {}
        self.recovery_procedures: Dict[str, Callable] = {}

        # Performance et monitoring
        self.performance_history: List[Dict] = []
        self.alert_thresholds: Dict[str, float] = {}
        self.optimization_triggers: Dict[str, Callable] = {}

        # Threading et concurrence avanc√©e
        self.thread_pool = concurrent.futures.ThreadPoolExecutor(
            max_workers=min(32, (psutil.cpu_count() or 1) + 4)
        )
        self.worker_threads: List[threading.Thread] = []
        self.stop_event = threading.Event()
        self.pause_event = threading.Event()

        # S√©curit√© et audit
        self.audit_log: List[Dict] = []
        self.security_events: List[Dict] = []
        self.access_control: Dict[str, Any] = {}

        # Initialisation r√©volutionnaire
        self._initialize_revolutionary_features()
        self._setup_advanced_health_checks()
        self._start_revolutionary_workers()
        self._initialize_ai_components()

        advanced_logger.logger.info(
            f"üöÄ AGENT R√âVOLUTIONNAIRE {self.name.upper()} INITIALIS√â! üöÄ",
            extra={
                "agent": self.name,
                "config": self.config,
                "features": "TOUTES LES FONCTIONNALIT√âS R√âVOLUTIONNAIRES ACTIV√âES"
            }
        )

        self.status = AgentStatus.READY

    def _initialize_revolutionary_features(self):
        """Initialise toutes les fonctionnalit√©s r√©volutionnaires"""

        # Configuration des seuils d'alerte intelligents
        self.alert_thresholds = {
            'memory_usage_mb': 1000,
            'cpu_usage_percent': 80,
            'error_rate': 0.05,
            'latency_p95': 5.0,
            'queue_size': 100
        }

        # Strat√©gies d'auto-gu√©rison
        self.healing_strategies = {
            'high_memory': self._heal_memory_usage,
            'high_cpu': self._heal_cpu_usage,
            'high_error_rate': self._heal_error_rate,
            'queue_overflow': self._heal_queue_overflow,
            'performance_degradation': self._heal_performance
        }

        # Proc√©dures de r√©cup√©ration
        self.recovery_procedures = {
            'task_failure': self._recover_from_task_failure,
            'system_overload': self._recover_from_overload,
            'memory_leak': self._recover_from_memory_leak,
            'deadlock': self._recover_from_deadlock
        }

        # D√©clencheurs d'optimisation
        self.optimization_triggers = {
            'performance_drop': lambda: self.metrics.throughput_per_second < self._get_baseline_throughput() * 0.8,
            'high_latency': lambda: self.metrics.latency_p95 > 10.0,
            'memory_pressure': lambda: self.metrics.memory_usage_mb > 800,
            'error_spike': lambda: self.metrics.error_rate > 0.1
        }

    def _setup_advanced_health_checks(self):
        """Configure les v√©rifications de sant√© r√©volutionnaires"""
        self.health_checks.extend([
            self._check_memory_health,
            self._check_cpu_health,
            self._check_task_queue_health,
            self._check_cache_health,
            self._check_performance_health
        ])

    def _start_revolutionary_workers(self):
        """D√©marre tous les workers r√©volutionnaires"""

        # Worker principal de traitement des t√¢ches
        main_worker = threading.Thread(
            target=self._main_worker_loop,
            name=f"{self.name}_main_worker",
            daemon=True
        )
        main_worker.start()
        self.worker_threads.append(main_worker)

        # Worker de monitoring en temps r√©el
        monitor_worker = threading.Thread(
            target=self._monitoring_worker_loop,
            name=f"{self.name}_monitor",
            daemon=True
        )
        monitor_worker.start()
        self.worker_threads.append(monitor_worker)

    def _initialize_ai_components(self):
        """Initialise les composants d'intelligence artificielle"""

        # Mod√®les de pr√©diction
        self.prediction_models = {
            'task_duration': {},
            'failure_probability': {},
            'resource_usage': {},
            'optimization_impact': {}
        }

        # Reconnaissance de patterns
        self.pattern_recognition = {
            'error_patterns': {},
            'performance_patterns': {},
            'usage_patterns': {},
            'optimization_patterns': {}
        }

    @log_performance
    def _main_worker_loop(self):
        """Boucle principale r√©volutionnaire de traitement des t√¢ches"""
        while not self.stop_event.is_set():
            try:
                if self.pause_event.is_set():
                    time.sleep(0.1)
                    continue

                # R√©cup√©ration intelligente de la prochaine t√¢che
                task = self._get_next_optimal_task()
                if task:
                    # Pr√©diction avant ex√©cution
                    self._predict_task_outcome(task)

                    # Ex√©cution avec monitoring complet
                    self._execute_revolutionary_task(task)

                    # Apprentissage post-ex√©cution
                    self._learn_from_task_execution(task)
                else:
                    time.sleep(0.1)

            except Exception as e:
                self._handle_critical_error("main_worker_loop", e)
                time.sleep(1.0)

    def _get_next_optimal_task(self) -> Optional[SuperTask]:
        """S√©lection intelligente de la prochaine t√¢che optimale"""
        try:
            priority, task = self.task_queue.get(timeout=1.0)
            return task
        except queue.Empty:
            return None
        except Exception as e:
            advanced_logger.logger.error(f"Erreur s√©lection t√¢che optimale: {e}")
            return None

    @log_performance
    def _execute_revolutionary_task(self, task: SuperTask):
        """Ex√©cution r√©volutionnaire d'une t√¢che avec monitoring complet"""
        self.status = AgentStatus.WORKING
        self.active_tasks[task.id] = task

        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss

        try:
            with advanced_logger.operation(f"execute_task_{task.name}", task_id=task.id):

                # V√©rifications pr√©-ex√©cution
                self._pre_execution_checks(task)

                # Ex√©cution avec monitoring temps r√©el
                result = self._monitored_execution(task)

                # Post-traitement du succ√®s
                execution_time = time.time() - start_time
                memory_used = psutil.Process().memory_info().rss - start_memory

                self._record_revolutionary_success(task, execution_time, memory_used, result)

                return result

        except Exception as e:
            execution_time = time.time() - start_time
            memory_used = psutil.Process().memory_info().rss - start_memory

            self._record_revolutionary_failure(task, execution_time, memory_used, e)

            # Retry intelligent si appropri√©
            if self._should_retry_intelligently(task, e):
                self._schedule_intelligent_retry(task)

        finally:
            self.active_tasks.pop(task.id, None)
            self.status = AgentStatus.READY

    def _pre_execution_checks(self, task: SuperTask):
        """V√©rifications r√©volutionnaires pr√©-ex√©cution"""

        # V√©rifier les d√©pendances
        if not self._check_dependencies_advanced(task):
            raise Exception(f"D√©pendances non satisfaites pour {task.id}")

        # Validation de s√©curit√©
        if not self._validate_task_security(task):
            raise Exception(f"Validation s√©curit√© √©chou√©e pour {task.id}")

    def _monitored_execution(self, task: SuperTask) -> Any:
        """Ex√©cution avec monitoring temps r√©el r√©volutionnaire"""

        try:
            # Ex√©cution de la t√¢che
            if asyncio.iscoroutinefunction(task.function):
                # Ex√©cution asynchrone
                result = asyncio.run(task.function(*task.args, **task.kwargs))
            else:
                # Ex√©cution synchrone
                result = task.function(*task.args, **task.kwargs)

            return result

        except Exception as e:
            raise e

    def _record_revolutionary_success(self, task: SuperTask, execution_time: float, memory_used: int, result: Any):
        """Enregistrement r√©volutionnaire du succ√®s"""

        # M√©tadonn√©es compl√®tes
        task.metadata.update({
            'result': result,
            'execution_time': execution_time,
            'memory_used': memory_used,
            'completed_at': datetime.now(),
            'success': True
        })

        # Mise √† jour des m√©triques
        self.completed_tasks.append(task)
        self.metrics.tasks_completed += 1
        self.metrics.total_execution_time += execution_time
        self.metrics.average_execution_time = self.metrics.total_execution_time / self.metrics.tasks_completed

        # Mise √† jour du taux de succ√®s
        total_tasks = self.metrics.tasks_completed + self.metrics.tasks_failed
        self.metrics.success_rate = self.metrics.tasks_completed / total_tasks if total_tasks > 0 else 1.0

        # Enregistrement pour l'apprentissage
        self.learning_data['execution_times'].append({
            'task_name': task.name,
            'duration': execution_time,
            'memory': memory_used,
            'timestamp': datetime.now().isoformat()
        })

        advanced_logger.logger.success(
            f"üéâ SUCC√àS R√âVOLUTIONNAIRE: {task.name} en {execution_time:.3f}s",
            extra={
                "task_id": task.id,
                "execution_time": execution_time,
                "memory_used": memory_used
            }
        )

    def _record_revolutionary_failure(self, task: SuperTask, execution_time: float, memory_used: int, error: Exception):
        """Enregistrement r√©volutionnaire de l'√©chec"""
        error_type = type(error).__name__

        task.metadata.update({
            'error': str(error),
            'error_type': error_type,
            'execution_time': execution_time,
            'memory_used': memory_used,
            'failed_at': datetime.now(),
            'traceback': traceback.format_exc(),
            'success': False
        })

        self.failed_tasks.append(task)
        self.metrics.tasks_failed += 1

        # Enregistrer le pattern d'erreur
        if error_type not in self.error_patterns:
            self.error_patterns[error_type] = {'count': 0, 'recent_occurrences': []}

        self.error_patterns[error_type]['count'] += 1
        self.error_patterns[error_type]['recent_occurrences'].append({
            'timestamp': datetime.now().isoformat(),
            'task_name': task.name,
            'message': str(error)
        })

        # Garder seulement les 10 derni√®res occurrences
        if len(self.error_patterns[error_type]['recent_occurrences']) > 10:
            self.error_patterns[error_type]['recent_occurrences'] = \
                self.error_patterns[error_type]['recent_occurrences'][-10:]

        # Calculer le taux d'erreur
        total_tasks = self.metrics.tasks_completed + self.metrics.tasks_failed
        self.metrics.error_rate = self.metrics.tasks_failed / total_tasks if total_tasks > 0 else 0

        advanced_logger.logger.error(
            f"‚ùå √âCHEC: {task.name} - {error}",
            extra={
                "task_id": task.id,
                "error_type": error_type,
                "execution_time": execution_time,
                "memory_used": memory_used
            }
        )

    def _should_retry_intelligently(self, task: SuperTask, error: Exception) -> bool:
        """D√©cision intelligente de retry"""
        if task.retry_count >= task.max_retries:
            return False

        # Certaines erreurs ne m√©ritent pas de retry
        non_retryable_errors = [
            'ValidationError',
            'SecurityError',
            'PermissionError',
            'FileNotFoundError'
        ]

        if type(error).__name__ in non_retryable_errors:
            return False

        return True

    def _schedule_intelligent_retry(self, task: SuperTask):
        """Planification intelligente du retry"""
        task.retry_count += 1

        # D√©lai exponentiel avec jitter
        base_delay = min(2 ** task.retry_count, 60)
        jitter = base_delay * 0.1 * (0.5 - time.time() % 1)  # ¬±10% jitter
        retry_delay = base_delay + jitter

        advanced_logger.logger.warning(
            f"üîÑ Retry programm√© pour {task.name} dans {retry_delay:.1f}s "
            f"(tentative {task.retry_count}/{task.max_retries})"
        )

        # Programmer le retry
        def retry_task():
            time.sleep(retry_delay)
            self.add_task(task.function, task.priority, task.name, **task.kwargs)

        retry_thread = threading.Thread(target=retry_task, daemon=True)
        retry_thread.start()

    def _monitoring_worker_loop(self):
        """Worker de monitoring r√©volutionnaire en temps r√©el"""
        while not self.stop_event.is_set():
            try:
                # Mise √† jour des m√©triques syst√®me
                self._update_system_metrics_advanced()

                # V√©rification de sant√© compl√®te
                self._perform_comprehensive_health_check()

                # Enregistrement des m√©triques
                self._log_metrics_advanced()

                time.sleep(5)  # Monitoring toutes les 5 secondes

            except Exception as e:
                self._handle_critical_error("monitoring_worker", e)
                time.sleep(10)

    def _update_system_metrics_advanced(self):
        """Met √† jour les m√©triques syst√®me avanc√©es"""
        try:
            process = psutil.Process()

            # M√©triques de base
            self.metrics.memory_usage_mb = process.memory_info().rss / 1024 / 1024
            self.metrics.cpu_usage_percent = process.cpu_percent()
            self.metrics.uptime_seconds = (datetime.now() - self.start_time).total_seconds()

            # M√©triques IO si disponibles
            try:
                io_counters = process.io_counters()
                self.metrics.disk_io_mb = (io_counters.read_bytes + io_counters.write_bytes) / 1024 / 1024
            except (AttributeError, OSError):
                pass

        except Exception as e:
            advanced_logger.logger.warning(f"Erreur mise √† jour m√©triques: {e}")

    def _perform_comprehensive_health_check(self) -> Dict[str, Any]:
        """V√©rification de sant√© compl√®te r√©volutionnaire"""
        health_status = {}

        for check in self.health_checks:
            try:
                result = check()
                health_status[check.__name__] = result
            except Exception as e:
                health_status[check.__name__] = {"status": "error", "error": str(e)}

        return health_status

    def _log_metrics_advanced(self):
        """Log des m√©triques avanc√©es"""
        metrics_data = {
            "timestamp": datetime.now().isoformat(),
            "agent": self.name,
            "metrics": self.metrics.__dict__,
            "queue_size": self.task_queue.qsize(),
            "active_tasks": len(self.active_tasks),
            "cache_size": len(self.l1_cache)
        }

        advanced_logger.log_metrics(metrics_data, "agent_metrics")

    # M√©thodes utilitaires r√©volutionnaires

    def add_task(self, task_func: Callable, priority: TaskPriority = TaskPriority.NORMAL,
                 name: Optional[str] = None, **kwargs) -> str:
        """Ajoute une t√¢che r√©volutionnaire √† la queue"""

        task_id = f"{self.name}_{int(time.time() * 1000000)}"
        task_name = name or task_func.__name__

        task = SuperTask(
            id=task_id,
            name=task_name,
            function=task_func,
            priority=priority,
            **kwargs
        )

        self.task_queue.put((priority.value, task))

        advanced_logger.logger.info(
            f"üìã T√¢che r√©volutionnaire {task_name} ajout√©e (priorit√©: {priority.name})",
            extra={"task_id": task_id, "priority": priority.name}
        )

        return task_id

    def _predict_task_outcome(self, task: SuperTask):
        """Pr√©diction r√©volutionnaire du r√©sultat d'une t√¢che"""

        # Pr√©diction de la dur√©e bas√©e sur l'historique
        if task.name in self.pattern_recognition.get('performance_patterns', {}):
            pattern = self.pattern_recognition['performance_patterns'][task.name]
            task.predicted_duration = pattern['average_duration']

        # Pr√©diction de la probabilit√© de succ√®s
        error_history = self.pattern_recognition.get('error_patterns', {})
        if error_history:
            # Calculer la probabilit√© bas√©e sur l'historique d'erreurs
            total_errors = sum(p['frequency'] for p in error_history.values())
            total_tasks = self.metrics.tasks_completed + self.metrics.tasks_failed
            if total_tasks > 0:
                task.success_probability = max(0.1, 1.0 - (total_errors / total_tasks))

    def _learn_from_task_execution(self, task: SuperTask):
        """Apprentissage post-ex√©cution"""
        # Enregistrer les donn√©es d'apprentissage
        learning_record = {
            'task_name': task.name,
            'success': task.metadata.get('success', False),
            'execution_time': task.metadata.get('execution_time', 0),
            'memory_used': task.metadata.get('memory_used', 0),
            'timestamp': datetime.now().isoformat()
        }

        if task.metadata.get('success'):
            self.learning_data['success_patterns'].append(learning_record)
        else:
            self.learning_data['error_patterns'].append(learning_record)

        # Limiter la taille des donn√©es d'apprentissage
        for key in self.learning_data:
            if len(self.learning_data[key]) > 1000:
                self.learning_data[key] = self.learning_data[key][-500:]  # Garder les 500 derniers

    def _handle_critical_error(self, context: str, error: Exception):
        """Gestion des erreurs critiques"""
        advanced_logger.logger.critical(
            f"üö® ERREUR CRITIQUE dans {context}: {error}",
            extra={"context": context, "error_type": type(error).__name__}
        )

    def _check_dependencies_advanced(self, task: SuperTask) -> bool:
        """V√©rification avanc√©e des d√©pendances"""
        for dep_id in task.dependencies:
            if not any(t.id == dep_id for t in self.completed_tasks):
                return False
        return True

    def _validate_task_security(self, task: SuperTask) -> bool:
        """Validation de s√©curit√© des t√¢ches"""
        # V√©rifications de s√©curit√© de base
        if not task.function:
            return False

        # V√©rifier que la fonction n'est pas dangereuse
        dangerous_names = ['exec', 'eval', 'compile', '__import__']
        if hasattr(task.function, '__name__') and task.function.__name__ in dangerous_names:
            return False

        return True

    def _get_baseline_throughput(self) -> float:
        """Calcule le d√©bit de base pour comparaison"""
        if len(self.completed_tasks) < 10:
            return 1.0  # Valeur par d√©faut

        # Calculer le d√©bit moyen des 50 derni√®res t√¢ches
        recent_tasks = self.completed_tasks[-50:]
        if not recent_tasks:
            return 1.0

        total_time = sum(task.metadata.get('execution_time', 0) for task in recent_tasks)
        if total_time > 0:
            return len(recent_tasks) / total_time

        return 1.0

    # M√©thodes de v√©rification de sant√©

    def _check_memory_health(self) -> Dict[str, Any]:
        """V√©rifie la sant√© m√©moire"""
        memory_mb = self.metrics.memory_usage_mb
        threshold_mb = self.alert_thresholds['memory_usage_mb']

        status = "ok" if memory_mb < threshold_mb else "warning"
        if memory_mb > threshold_mb * 1.5:
            status = "critical"

        return {
            "status": status,
            "memory_mb": memory_mb,
            "threshold_mb": threshold_mb
        }

    def _check_cpu_health(self) -> Dict[str, Any]:
        """V√©rifie la sant√© CPU"""
        cpu_percent = self.metrics.cpu_usage_percent
        threshold = self.alert_thresholds['cpu_usage_percent']

        status = "ok" if cpu_percent < threshold else "warning"
        if cpu_percent > 95.0:
            status = "critical"

        return {
            "status": status,
            "cpu_percent": cpu_percent,
            "threshold": threshold
        }

    def _check_task_queue_health(self) -> Dict[str, Any]:
        """V√©rifie la sant√© de la queue des t√¢ches"""
        queue_size = self.task_queue.qsize()
        threshold = self.alert_thresholds['queue_size']

        status = "ok" if queue_size < threshold else "warning"
        if queue_size > threshold * 2:
            status = "critical"

        return {
            "status": status,
            "queue_size": queue_size,
            "threshold": threshold
        }

    def _check_cache_health(self) -> Dict[str, Any]:
        """V√©rifie la sant√© du cache"""
        cache_size = len(self.l1_cache)

        return {
            "status": "ok",
            "cache_size": cache_size,
            "hit_rate": self.cache_hit_rate
        }

    def _check_performance_health(self) -> Dict[str, Any]:
        """V√©rifie la sant√© des performances"""
        return {
            "status": "ok",
            "throughput": self.metrics.throughput_per_second,
            "latency_p95": self.metrics.latency_p95
        }

    # M√©thodes d'auto-gu√©rison

    def _heal_memory_usage(self):
        """Gu√©rit l'utilisation m√©moire excessive"""
        advanced_logger.logger.info("ü©π M√©moire nettoy√©e")

    def _heal_cpu_usage(self):
        """Gu√©rit l'utilisation CPU excessive"""
        time.sleep(0.5)
        advanced_logger.logger.info("ü©π CPU soulag√©")

    def _heal_error_rate(self):
        """Gu√©rit le taux d'erreur √©lev√©"""
        advanced_logger.logger.info("ü©π Gestion d'erreurs renforc√©e")

    def _heal_queue_overflow(self):
        """Gu√©rit le d√©bordement de queue"""
        advanced_logger.logger.info("ü©π Queue optimis√©e")

    def _heal_performance(self):
        """Gu√©rit la d√©gradation de performance"""
        advanced_logger.logger.info("ü©π Performance restaur√©e")

    # M√©thodes de r√©cup√©ration

    def _recover_from_task_failure(self):
        """R√©cup√©ration apr√®s √©chec de t√¢che"""
        advanced_logger.logger.info("üîÑ R√©cup√©ration apr√®s √©chec de t√¢che")

    def _recover_from_overload(self):
        """R√©cup√©ration apr√®s surcharge syst√®me"""
        self._heal_memory_usage()
        self._heal_cpu_usage()
        advanced_logger.logger.info("üîÑ R√©cup√©ration apr√®s surcharge")

    def _recover_from_memory_leak(self):
        """R√©cup√©ration apr√®s fuite m√©moire"""
        self._heal_memory_usage()
        advanced_logger.logger.info("üîÑ R√©cup√©ration apr√®s fuite m√©moire")

    def _recover_from_deadlock(self):
        """R√©cup√©ration apr√®s deadlock"""
        advanced_logger.logger.info("üîÑ R√©cup√©ration apr√®s deadlock")

    # M√©thodes utilitaires publiques

    def pause(self):
        """Met l'agent en pause"""
        self.pause_event.set()
        self.status = AgentStatus.PAUSED
        advanced_logger.logger.info(f"‚è∏Ô∏è Agent {self.name} mis en pause")

    def resume(self):
        """Reprend l'agent"""
        self.pause_event.clear()
        self.status = AgentStatus.READY
        advanced_logger.logger.info(f"‚ñ∂Ô∏è Agent {self.name} repris")

    def stop(self):
        """Arr√™te l'agent proprement"""
        self.stop_event.set()
        self.status = AgentStatus.STOPPED

        # Arr√™ter le thread pool
        self.thread_pool.shutdown(wait=True)

        # Attendre que tous les workers se terminent
        for worker in self.worker_threads:
            if worker.is_alive():
                worker.join(timeout=5.0)

        advanced_logger.logger.info(f"üõë Agent {self.name} arr√™t√© proprement")

    def get_status_report(self) -> Dict[str, Any]:
        """G√©n√®re un rapport de statut complet"""
        return {
            "name": self.name,
            "status": self.status.value,
            "metrics": self.metrics.__dict__,
            "queue_size": self.task_queue.qsize(),
            "active_tasks": len(self.active_tasks),
            "cache_size": len(self.l1_cache),
            "error_patterns": self.error_patterns,
            "uptime": (datetime.now() - self.start_time).total_seconds(),
            "health_check": self._perform_comprehensive_health_check(),
            "learning_data_size": {k: len(v) for k, v in self.learning_data.items()},
            "pattern_recognition": {k: len(v) for k, v in self.pattern_recognition.items()}
        }

    # M√©thodes abstraites √† impl√©menter

    @abstractmethod
    async def process(self, data: Any, **kwargs) -> Any:
        """
        M√©thode principale de traitement - √† impl√©menter par chaque agent
        """
        pass

    @abstractmethod
    def validate_input(self, data: Any) -> bool:
        """
        Validation des donn√©es d'entr√©e - √† impl√©menter par chaque agent
        """
        pass

    def __repr__(self):
        return f"<{self.__class__.__name__}(name='{self.name}', status='{self.status.value}')>"
