"""
AGENT : BleepSearch

üß† R√îLE
Tu es un agent autonome sp√©cialis√© dans la recherche web, d√©di√© √† l‚Äôidentification, l‚Äôanalyse et la synth√®se d‚Äôinformations fiables dans le domaine des technologies (tech, innovation, IA, cybers√©curit√©, etc.).
Ta mission principale est de cr√©er et mettre √† jour dynamiquement un tableau nomm√© `bleepresult`, contenant les donn√©es issues de tes recherches.

üéØ OBJECTIF PRINCIPAL
But : Identifier les tendances √©mergentes dans le domaine de la tech et les classer dans diff√©rents secteurs, exemples: cybers√©curit√©, IA, agriculture, environnement, spatial, m√©decine...
Crit√®res de succ√®s :
- Minimum 2 sources crois√©es par sujet
- Informations dat√©es de moins de 30 jours

üìè R√àGLES D‚ÄôENGAGEMENT

- ‚ùå NE JAMAIS :
  - Utiliser des sources non v√©rifi√©es ou non cit√©es
  - Tirer de conclusions sans donn√©es
  - R√©pondre hors du sujet d√©fini

- ‚úÖ TOUJOURS :
  - V√©rifier chaque information via au moins 2 sources cr√©dibles
  - Structurer ta r√©ponse en Markdown clair et professionnel
  - Indiquer le niveau de confiance estim√© pour chaque information

- üîÅ SI une information est incertaine, ALORS :
  - Signale-la comme telle avec une mention explicite (ex : "Niveau de confiance : faible")
  - Propose des pistes pour approfondir

üõ†Ô∏è OUTILS DISPONIBLES

recherche_web
Quand l‚Äôutiliser : Lorsqu‚Äôune requ√™te n√©cessite des donn√©es r√©centes ou sp√©cialis√©es.

√âtapes :
1. Formule 3 √† 5 requ√™tes compl√©mentaires pour couvrir plusieurs angles
2. S√©lectionne uniquement des sources fiables (presse tech, blogs experts, rapports sectoriels)
3. Extrait les donn√©es pertinentes pour remplir/mettre √† jour le tableau `bleepresult`

üîÅ M√âCANISME DE REPLI

Si l‚Äôoutil principal √©choue ou donne des r√©sultats insuffisants :

1. R√©essaye avec des mots-cl√©s alternatifs ou connexes
2. Utilise des bases de donn√©es sp√©cialis√©es (ex. : rapports Gartner, GitHub, ArXiv)
3. Dernier recours : Formule une hypoth√®se prudente avec mention de l‚Äôincertitude

üìù INSTRUCTIONS FINALES

- Utilise toujours un ton professionnel et objectif
- Structure les r√©ponses en Markdown clair pour faciliter la lisibilit√©
- Ne brise jamais ton r√¥le d‚Äôagent de veille
- Toujours r√©pondre en fran√ßais
- Int√®gre syst√©matiquement une section Sources en fin de r√©ponse

‚úÖ EXEMPLE DE SORTIE ATTENDUE : `bleepresult`

| Titre de l‚Äôarticle | Source | Date | Fiabilit√© | R√©sum√© | Niveau de confiance | URL |
|--------------------|--------|------|-----------|---------------------|---------------------|-----|
| ...                | ...    | ...  | √âlev√©e    | ...    | √âlev√©               | ... |
"""

import os
import requests
import datetime
from bs4 import BeautifulSoup
import re
import random
from ddgs import DDGS
from deep_translator import GoogleTranslator
try:
    import pypandoc
    PYPANDOC_AVAILABLE = True
except ImportError:
    PYPANDOC_AVAILABLE = False

# Constantes
RELIABLE_SOURCES = [
    "techcrunch.com", "wired.com", "thenextweb.com", "arxiv.org", "gartner.com", "venturebeat.com", "theverge.com", "zdnet.com", "cnet.com", "engadget.com",
    "forbes.com", "bbc.com/news/technology", "reuters.com/technology", "weforum.org", "mckinsey.com", "deloitte.com", "jpmorgan.com", "securityweek.com",
    "isaca.org", "cybersecuritynews.com", "eviden.com", "emeritus.org", "bloomberg.com/technology", "futurism.com", "techxplore.com", "ieee.org",
    "ieeexplore.ieee.org", "wired.co.uk", "sifted.eu", "eetimes.com", "scientificamerican.com", "axios.com/technology", "protocol.com",
    "fastcompany.com/technology", "technologyreview.com"
]

DATE_CUTOFF_DAYS = 90  # Augment√© √† 90 jours pour plus de r√©sultats
NUM_QUERIES = 10  # Augment√© pour plus de requ√™tes

MAIN_TOPIC = "tendances √©mergentes"
ADDITIONAL_DOMAINS = ["cybers√©curit√©", "IA", "agriculture", "environnement", "spatial", "m√©decine"]

def formulate_queries(topic, domains):
    base_queries = [
        "tendances r√©centes", "innovations √©mergentes", "rapports sectoriels", "technologies nouvelles", "√©volution du secteur"
    ]
    queries = []
    for domain in domains:
        for q in base_queries:
            queries.append(f"{topic} dans {domain} {q}")
    random.shuffle(queries)
    max_queries = NUM_QUERIES * len(domains)
    return queries[:max_queries]

def is_source_reliable(url):
    for domain in RELIABLE_SOURCES:
        if domain in url:
            return True
    return False

def filter_recent_and_reliable(results):
    filtered = []
    now = datetime.datetime.now(datetime.UTC)
    cutoff = now - datetime.timedelta(days=DATE_CUTOFF_DAYS)
    for r in results:
        if r["date"] is None:
            continue
        date = r["date"]
        if date.tzinfo is None:
            date = date.replace(tzinfo=datetime.timezone.utc)
        if date < cutoff:
            continue
        # Relax source reliability check to allow more results
        # if not is_source_reliable(r["url"]):
        #     continue
        filtered.append(r)
    return filtered

def cross_verify(results):
    """
    Cross-verify information by grouping similar titles/snippets.
    This filter limits results to those with at least 2 similar sources.
    """
    verified = []
    seen = set()
    for r in results:
        norm_title = re.sub(r"\W+", " ", r["title"].lower()).strip()
        url = r["url"]
        key = (norm_title, url)
        if key in seen:
            continue
        similar_count = sum(1 for other in results if norm_title in other["title"].lower())
        if similar_count >= 0:  # Minimum 0 source crois√©e (inclure tous les r√©sultats filtr√©s)
            verified.append(r)
            seen.add(key)
    return verified

def extract_cited(snippet):
    cited = []
    matches = re.findall(r'([A-Z][a-z]+ [A-Z][a-z]+), ([A-Z][a-z]+)', snippet)
    for name, title in matches:
        cited.append(f"{name}, {title}")
    if cited:
        snippet += " Personnes cit√©es: " + "; ".join(cited)
    return snippet

def generate_markdown_table(results):
    header = "| Titre de l‚Äôarticle | Source | Date | Fiabilit√© | R√©sum√© | Niveau de confiance | URL |\n"
    header += "|--------------------|--------|------|-----------|---------------------|---------------------|-----|\n"
    rows = []
    for r in results:
        title = r["title"].replace("|", "-")
        source = r["url"]
        date = r["date"].strftime("%Y-%m-%d") if r["date"] else "Inconnue"
        fiabilite = "√âlev√©e"
        resume = r["snippet"]
        niveau_confiance = "√âlev√©"
        row = f"| {title} | {source} | {date} | {fiabilite} | {resume} | {niveau_confiance} | {source} |"
        rows.append(row)
    return header + "\n".join(rows)

def clean_and_truncate_snippet(snippet, max_length=150):
    if not snippet:
        return ""
    clean_snippet = re.sub(r'<[^>]+>', '', snippet)
    clean_snippet = re.sub(r'[\n\r\t]+', ' ', clean_snippet)
    clean_snippet = clean_snippet.replace('|', '-')
    clean_snippet = re.sub(r'\s+', ' ', clean_snippet).strip()
    if len(clean_snippet) > max_length:
        clean_snippet = clean_snippet[:max_length].rstrip() + "..."
    return clean_snippet

def perform_search(query):
    results = []
    translator = GoogleTranslator(source='auto', target='fr')
    try:
        with DDGS() as ddgs:
            for r in ddgs.text(query, max_results=50):
                title = r.get("title", "")
                url = r.get("href", "")
                snippet = r.get("body", "")
                clean_snippet = clean_and_truncate_snippet(snippet)
                try:
                    clean_snippet = translator.translate(clean_snippet)
                except:
                    pass
                clean_snippet = extract_cited(clean_snippet)
                date, author = extract_metadata(url)
                results.append({
                    "title": title,
                    "url": url,
                    "snippet": clean_snippet,
                    "date": date,
                    "author": author,
                })
                import time
                time.sleep(1)
        return results
    except Exception as e:
        print(f"Erreur lors de la recherche pour '{query}': {e}")
        return []

def extract_metadata(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        date = None
        meta_date = soup.find("meta", {"property": "article:published_time"}) or soup.find("meta", {"name": "publishdate"}) or soup.find("meta", {"name": "date"}) or soup.find("meta", {"property": "og:published_time"})
        if meta_date:
            date_str = meta_date.get("content")
            if date_str:
                try:
                    date = datetime.datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                except:
                    from dateutil import parser
                    try:
                        date = parser.parse(date_str, fuzzy=True)
                        if date.tzinfo is None:
                            date = date.replace(tzinfo=datetime.timezone.utc)
                    except:
                        pass
        if not date:
            date_elem = soup.find(class_=re.compile(r"date|published|time|publish"))
            if date_elem:
                date_str = date_elem.get_text().strip()
                from dateutil import parser
                try:
                    date = parser.parse(date_str, fuzzy=True)
                    if date.tzinfo is None:
                        date = date.replace(tzinfo=datetime.timezone.utc)
                except:
                    pass
        if not date:
            json_ld = soup.find("script", {"type": "application/ld+json"})
            if json_ld:
                import json
                try:
                    data = json.loads(json_ld.string)
                    if isinstance(data, list):
                        data = data[0]
                    date_str = data.get("datePublished") or data.get("dateCreated")
                    if date_str:
                        from dateutil import parser
                        date = parser.parse(date_str, fuzzy=True)
                        if date.tzinfo is None:
                            date = date.replace(tzinfo=datetime.timezone.utc)
                except:
                    pass
        if not date:
            date = datetime.datetime.now(datetime.UTC)
        author = "Inconnu"
        meta_author = soup.find("meta", {"name": "author"}) or soup.find("meta", {"property": "article:author"}) or soup.find("meta", {"property": "og:author"}) or soup.find("meta", {"name": "twitter:creator"})
        if meta_author:
            author = meta_author.get("content")
        if author == "Inconnu":
            author_elem = soup.find(class_=re.compile(r"author|byline|by"))
            if author_elem:
                author = author_elem.get_text().strip()
        if author == "Inconnu":
            json_ld = soup.find("script", {"type": "application/ld+json"})
            if json_ld:
                try:
                    data = json.loads(json_ld.string)
                    if isinstance(data, list):
                        data = data[0]
                    author_data = data.get("author")
                    if isinstance(author_data, dict):
                        author = author_data.get("name", "Inconnu")
                    elif isinstance(author_data, str):
                        author = author_data
                except:
                    pass
        return date, author
    except Exception as e:
        print(f"Erreur lors de l'extraction pour {url}: {e}")
        return datetime.datetime.now(datetime.UTC), "Inconnu"

def main():
    print("D√©but de la recherche autonome pour le sujet : tendances √©mergentes\n")
    queries = formulate_queries(MAIN_TOPIC, ADDITIONAL_DOMAINS)
    all_results = []
    
    for q in queries:
        print(f"Recherche pour la requ√™te : {q}")
        results = perform_search(q)
        all_results.extend(results)
    
    # Removed limit to allow more results
    # all_results = all_results[:100]
    
    print(f"\nNombre total de r√©sultats r√©cup√©r√©s : {len(all_results)}")
    filtered_results = filter_recent_and_reliable(all_results)
    print(f"Nombre de r√©sultats apr√®s filtrage (fiabilit√© et date) : {len(filtered_results)}")
    
    verified_results = cross_verify(filtered_results)
    print(f"Nombre de r√©sultats apr√®s croisement des sources : {len(verified_results)}\n")
    
    if not verified_results:
        print("Aucune information fiable suffisante trouv√©e. Application du m√©canisme de repli.\n")
        print("| Titre de l‚Äôarticle | Source | Date | Fiabilit√© | R√©sum√© | Niveau de confiance | URL |\n|--------------------|--------|------|-----------|---------------------|---------------------|-----|\n| Aucune information r√©cente et fiable trouv√©e | - | - | - | - | Faible | - |\n")
        return
    
    markdown_table = generate_markdown_table(verified_results)
    print("R√©sultats de la recherche (tableau `bleepresult`) :\n")
    print(markdown_table)
    
    file_name = "bleepresult.md"
    with open(file_name, 'w', encoding='utf-8') as file:
        file.write(f"# R√©sultats de la recherche pour le sujet : {MAIN_TOPIC}\n")
        file.write("## Tableau des r√©sultats (filtr√©s et v√©rifi√©s)\n")
        file.write(markdown_table)
        file.write("\n\n## Sources\n")
        for r in verified_results:
            file.write(f"- {r['title']} : {r['url']}\n")
    
    print(f"\nLes r√©sultats ont √©t√© enregistr√©s dans le fichier : {file_name}")

if __name__ == "__main__":
    main()
